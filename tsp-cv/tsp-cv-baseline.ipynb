{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/nazmi/038f6163a558e903227f48e20a47e709/tsp-cv-baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1vj-5tnqyks"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:01:16.847974Z",
          "start_time": "2022-04-11T21:01:12.120393Z"
        },
        "id": "njpmNMZLqykt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:01:17.771417Z",
          "start_time": "2022-04-11T21:01:16.848975Z"
        },
        "id": "SsbUn-Z_qykw"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
        "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version: {tf.keras.__version__}\")\n",
        "print()\n",
        "print(f\"Python {sys.version}\")\n",
        "print(f\"Pandas {pd.__version__}\")\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        print(tf.config.experimental.get_device_details(gpu))\n",
        "else:\n",
        "    print(\"GPU is NOT AVAILABLE\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUo1dXkjqyky"
      },
      "source": [
        "# Fetch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:01:18.982932Z",
          "start_time": "2022-04-11T21:01:18.967933Z"
        },
        "id": "AbKpzuFKqykz"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    PATH = \"/content/\"\n",
        "else:\n",
        "    PATH = \".\"\n",
        "\n",
        "PATH_INPUT = os.path.join(PATH,\"input/\")\n",
        "PATH_DATASET = os.path.join(PATH_INPUT, \"dataset/\")   \n",
        "PATH_TRAIN = os.path.join(PATH_DATASET,\"train.csv\")\n",
        "PATH_TEST = os.path.join(PATH_DATASET,\"test.csv\")\n",
        "\n",
        "if not os.path.exists(PATH_DATASET):\n",
        "    ! kaggle competitions download -c tsp-cv\n",
        "    with zipfile.ZipFile(\"tsp-cv.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(path=PATH_DATASET)\n",
        "        zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:01:19.030441Z",
          "start_time": "2022-04-11T21:01:18.983933Z"
        },
        "id": "gxDHnzpOqyk0"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(PATH_TRAIN)\n",
        "test_df = pd.read_csv(PATH_TEST)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fb6Bp0Lqyk0"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:01:19.486399Z",
          "start_time": "2022-04-11T21:01:19.031443Z"
        },
        "id": "VOFyhgGXqyk1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df[\"filename\"].values, train_df[\"distance\"].values,\n",
        "                                                    test_size=0.2, train_size=0.8,\n",
        "                                                    random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMw7J-fJqyk1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0,1000))\n",
        "y_train_scaled = minmax_scaler.fit_transform(y_train.reshape(-1, 1)).squeeze()\n",
        "y_test_scaled = minmax_scaler.transform(y_test.reshape(-1, 1)).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szby0S0Cqyk1"
      },
      "source": [
        "## Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:01:19.627080Z",
          "start_time": "2022-04-11T21:01:19.487399Z"
        },
        "id": "AWHBzh2Hqyk2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "rand = random.randrange(len(X_train))\n",
        "img = mpimg.imread(os.path.join(PATH_DATASET, X_train[rand]))\n",
        "plt.imshow(img);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kJY_MT2qyk2"
      },
      "source": [
        "# Preprocesssing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5pOJW6Gqyk2"
      },
      "source": [
        "## tf.data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:07:34.645952Z",
          "start_time": "2022-04-11T21:07:34.469234Z"
        },
        "id": "s860lhSbqyk3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "IMG_SIZE = 320\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "@tf.function\n",
        "def read_image(image):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      image = tf.io.read_file(PATH_DATASET + image)\n",
        "      image = tf.io.decode_jpeg(image, channels=3, dct_method='INTEGER_ACCURATE')\n",
        "      image = tf.image.resize_with_pad(image, IMG_SIZE, IMG_SIZE)\n",
        "      return image\n",
        "\n",
        "\n",
        "def rescale(image):\n",
        "\n",
        "    rescale = tf.keras.Sequential([\n",
        "        layers.Rescaling(scale=1./127.5, offset=-1)\n",
        "    ])\n",
        "\n",
        "    image = rescale(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def augmentation(image):\n",
        "\n",
        "    augment = tf.keras.Sequential([\n",
        "        layers.RandomFlip()\n",
        "    ])\n",
        "\n",
        "    image = augment(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def prepare(ds, shuffle=False, repeat=False, cache=False, scale=False, augment=False, batch_size=BATCH_SIZE):\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=1000)\n",
        "\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "\n",
        "    ds = ds.map(lambda x, y: (read_image(x), y),\n",
        "                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = ds.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "    if augment:\n",
        "        ds = ds.map(lambda x, y: (augmentation(x), y),\n",
        "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if scale:\n",
        "        ds = ds.map(lambda x, y: (rescale(x), y),\n",
        "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:07:40.276181Z",
          "start_time": "2022-04-11T21:07:39.897989Z"
        },
        "id": "URS7-tFLqyk3"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_data = prepare(train_data, scale=True, shuffle=True,augment=True)\n",
        "\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "valid_data = prepare(valid_data, scale=True)\n",
        "\n",
        "train_data_scaled = tf.data.Dataset.from_tensor_slices((X_train, y_train_scaled))\n",
        "train_data_scaled = prepare(train_data_scaled, scale=True, shuffle=True,augment=True,cache=True)\n",
        "\n",
        "valid_data_scaled = tf.data.Dataset.from_tensor_slices((X_test, y_test_scaled))\n",
        "valid_data_scaled = prepare(valid_data_scaled, scale=True,cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:07:40.587744Z",
          "start_time": "2022-04-11T21:07:40.277182Z"
        },
        "id": "4XBbJsghqyk3"
      },
      "outputs": [],
      "source": [
        "for image, label in train_data.take(1):\n",
        "    print(image.shape, label.shape)\n",
        "    print(image.dtype)\n",
        "    print(f\" Min: {tf.reduce_min(image[0])} Max: {tf.reduce_max(image[0])}\")\n",
        "    plt.imshow(tf.cast((image[0]+1)/2,dtype=tf.float32))\n",
        "    print(f\" Distance: {label[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJCx9dVVqyk4"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:07:40.743238Z",
          "start_time": "2022-04-11T21:07:40.682236Z"
        },
        "id": "C8i9inQgqyk4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "\n",
        "input_shape = (IMG_SIZE, IMG_SIZE,3)\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\", dtype=tf.float32)\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(2048, activation=\"relu\")(x)\n",
        "x = layers.Dense(2048, activation=\"relu\")(x)\n",
        "\n",
        "outputs = layers.Dense(1,activation=\"linear\", name=\"output_layer\",dtype=tf.float32)(x)\n",
        "model = models.Model(inputs, outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:07:41.349666Z",
          "start_time": "2022-04-11T21:07:40.744238Z"
        },
        "id": "48QE5T1sqyk4"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-11T21:07:44.213071Z",
          "start_time": "2022-04-11T21:07:41.350667Z"
        },
        "id": "gJnSpLstqyk5"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "LEARNING_RATE = 3e-3\n",
        "EPOCHS = 5 #EDIT HERE\n",
        "\n",
        "\n",
        "stop_callback = callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4,\n",
        "                                        patience=25, verbose=1, mode='auto',\n",
        "                                        restore_best_weights=True)\n",
        "\n",
        "lr_callback = callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                          factor=0.2, min_lr=1e-9, patience=2)\n",
        "\n",
        "checkpoint_callback = callbacks.ModelCheckpoint(\"models/baseline/\",monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "              loss=\"mse\",\n",
        "              metrics=[RootMeanSquaredError(name=\"rmse\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2022-04-11T21:07:39.902Z"
        },
        "id": "nW8PAgyzqyk5"
      },
      "outputs": [],
      "source": [
        "history = (\n",
        "    model.fit(\n",
        "        train_data_scaled,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=valid_data_scaled,\n",
        "        callbacks=[stop_callback, lr_callback, checkpoint_callback]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iq--NV5qyk5"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(history):\n",
        "    \"\"\"\n",
        "    Returns separate loss curves for training and validation metrics.\n",
        "    Args:\n",
        "      history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
        "    \"\"\"\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    accuracy = history.history['rmse']\n",
        "    val_accuracy = history.history['val_rmse']\n",
        "\n",
        "    epochs = range(len(history.history['loss']))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.plot(epochs, loss, label='training_loss')\n",
        "    plt.plot(epochs, val_loss, label='val_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    \n",
        "plot_loss_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXozZzYcqyk6"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5k3CN7Wqyk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, max_error\n",
        "\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    max_err = max_error(y_true, y_pred)\n",
        "\n",
        "    model_results = {\"Mean Absolute Error\": mae,\n",
        "                     \"Mean Square Error\": mse,\n",
        "                     \"Root Mean Square Error\": np.sqrt(mse),\n",
        "                     \"Max Error\": max_err}\n",
        "\n",
        "    return model_results\n",
        "\n",
        "def calculate_results_scaled(y_true, y_pred):\n",
        "    inv_y_true = minmax_scaler.inverse_transform(y_true.reshape(-1, 1))\n",
        "    inv_y_pred = minmax_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
        "    model_results = calculate_results(inv_y_true, inv_y_pred)\n",
        "    \n",
        "    return model_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ydhk9wnIqyk6"
      },
      "outputs": [],
      "source": [
        "model_pred_probs = model.predict(valid_data_scaled)\n",
        "model_results = calculate_results_scaled(y_test_scaled, model_pred_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3bSo2ZRqyk6"
      },
      "outputs": [],
      "source": [
        "model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnEJVJGNqyk6"
      },
      "source": [
        "# Top 10% Wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHnTFaiZqyk7"
      },
      "outputs": [],
      "source": [
        "inv_y_test = minmax_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).squeeze()\n",
        "inv_y_pred = minmax_scaler.inverse_transform(model_pred_probs.reshape(-1, 1)).squeeze()\n",
        "error = (inv_y_test - inv_y_pred).squeeze()\n",
        "square_error = np.square(error)\n",
        "\n",
        "validation_df = pd.DataFrame({'true': inv_y_test,\n",
        "                             'pred': inv_y_pred,\n",
        "                             'error': error,\n",
        "                             'square_error': square_error})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFIxSdYKqyk7"
      },
      "outputs": [],
      "source": [
        "top_100_wrong = validation_df.sort_values(\"error\", ascending=False).head(100)\n",
        "top_100_wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej3eUlIjqyk7"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqdYcg1Mqyk7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare(ds, shuffle=False, repeat=False, cache=False,scale=False,augment=False):\n",
        "        \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=1000)\n",
        "\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "\n",
        "    ds = ds.map(read_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = ds.batch(BATCH_SIZE,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        ds = ds.map(augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "    if scale:\n",
        "        ds = ds.map(rescale, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2022-04-11T21:07:39.903Z"
        },
        "id": "qlaD9eQtqyk8"
      },
      "outputs": [],
      "source": [
        "test_data = tf.data.Dataset.from_tensor_slices(test_df[\"filename\"].values)\n",
        "test_data = prepare(test_data, scale=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2022-04-11T21:07:39.904Z"
        },
        "id": "YOO_MkGkqyk8"
      },
      "outputs": [],
      "source": [
        "model_predictions = model.predict(test_data)\n",
        "model_predictions = minmax_scaler.inverse_transform(model_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxvJDbeOqyk8"
      },
      "outputs": [],
      "source": [
        "model_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2022-04-11T21:07:39.905Z"
        },
        "id": "ZA7jYKOyqyk8"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame({\"id\": test_df[\"id\"], \"distance\": model_predictions.squeeze()})\n",
        "\n",
        "submission_df.to_csv(\"submission-baseline.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "tsp-cv-baseline.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0a38d37dc6a45ce77d3fdf67eee06a9a65b273241b27a8c0a288784bfb7a982d"
    },
    "kernelspec": {
      "display_name": "Python 3.8 (tensorflow)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

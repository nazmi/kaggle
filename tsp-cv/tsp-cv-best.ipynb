{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/nazmi/65867c9d99fc81fe1f63803c507cb74d/tsp-cv-best.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1vj-5tnqyks"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njpmNMZLqykt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsbUn-Z_qykw"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
        "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
        "tf.random.set_seed(42)\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version: {tf.keras.__version__}\")\n",
        "print()\n",
        "print(f\"Python {sys.version}\")\n",
        "print(f\"Pandas {pd.__version__}\")\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        print(tf.config.experimental.get_device_details(gpu))\n",
        "else:\n",
        "    print(\"GPU is NOT AVAILABLE\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUo1dXkjqyky"
      },
      "source": [
        "# Fetch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbKpzuFKqykz"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    PATH = \"/content/\"\n",
        "else:\n",
        "    PATH = \".\"\n",
        "\n",
        "PATH_INPUT = os.path.join(PATH,\"input/\")\n",
        "PATH_DATASET = os.path.join(PATH_INPUT, \"dataset/\")   \n",
        "PATH_TRAIN = os.path.join(PATH_INPUT,\"train.pkl\")\n",
        "PATH_TEST = os.path.join(PATH_INPUT,\"test.pkl\")\n",
        "\n",
        "if not os.path.exists(PATH_DATASET):\n",
        "    ! kaggle competitions download -c tsp-cv\n",
        "    with zipfile.ZipFile(\"tsp-cv.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(path=PATH_DATASET)\n",
        "        zip_ref.close()\n",
        "\n",
        "if not os.path.exists(PATH_TRAIN):\n",
        "    if IN_COLAB:\n",
        "        ! python /content/src/dataset_parser.py\n",
        "    else:\n",
        "        ! python /src/dataset_parser.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxDHnzpOqyk0"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_pickle(PATH_TRAIN)\n",
        "test_df = pd.read_pickle(PATH_TEST)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fb6Bp0Lqyk0"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOFyhgGXqyk1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df[\"image_hist\"].to_list(), train_df[\"distance\"].values,\n",
        "                                                    test_size=0.15, train_size=0.85, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5pOJW6Gqyk2"
      },
      "source": [
        "# tf.data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s860lhSbqyk3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "def prepare(ds, shuffle=False, repeat=False, cache=False, batch_size=32):\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=1000)\n",
        "\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "\n",
        "    ds = ds.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URS7-tFLqyk3"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 24\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_data = prepare(train_data,batch_size=BATCH_SIZE,shuffle=True,cache=True )\n",
        "\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "valid_data = prepare(valid_data,batch_size=BATCH_SIZE,cache=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec26ANKwiEIj"
      },
      "outputs": [],
      "source": [
        "for x,y in train_data.take(1):\n",
        "    print(x.shape, y.shape)\n",
        "    print(x[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJCx9dVVqyk4"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8i9inQgqyk4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "fc_layer = 1504\n",
        "ratio = 2\n",
        "input_shape = (3,256,)\n",
        "\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\", dtype=tf.float32)\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(fc_layer, activation=\"relu\")(x)\n",
        "x = layers.Dense(fc_layer//ratio, activation=\"relu\")(x)\n",
        "\n",
        "outputs = layers.Dense(1,activation=\"linear\", name=\"output_layer\",dtype=tf.float32)(x)\n",
        "model = models.Model(inputs, outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48QE5T1sqyk4"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJnSpLstqyk5"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "LEARNING_RATE = 0.00022186103371585797\n",
        "EPOCHS = 5 #EDIT THIS\n",
        "\n",
        "\n",
        "stop_callback = callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4,\n",
        "                                        patience=25, verbose=1, mode='auto',\n",
        "                                        restore_best_weights=True)\n",
        "\n",
        "lr_callback = callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                          factor=0.2, min_lr=1e-10, patience=2)\n",
        "\n",
        "checkpoint_callback = callbacks.ModelCheckpoint(\"models/best/\",monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "              loss=\"mse\",\n",
        "              metrics=[RootMeanSquaredError(name=\"rmse\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW8PAgyzqyk5"
      },
      "outputs": [],
      "source": [
        "history = (\n",
        "    model.fit(\n",
        "        train_data,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=valid_data,\n",
        "        callbacks=[stop_callback, lr_callback,checkpoint_callback]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iq--NV5qyk5"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(history):\n",
        "    \"\"\"\n",
        "    Returns separate loss curves for training and validation metrics.\n",
        "    Args:\n",
        "      history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
        "    \"\"\"\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    accuracy = history.history['rmse']\n",
        "    val_accuracy = history.history['val_rmse']\n",
        "\n",
        "    epochs = range(len(history.history['loss']))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.plot(epochs, loss, label='training_loss')\n",
        "    plt.plot(epochs, val_loss, label='val_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    \n",
        "plot_loss_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXozZzYcqyk6"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5k3CN7Wqyk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, max_error\n",
        "\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    max_err = max_error(y_true, y_pred)\n",
        "\n",
        "    model_results = {\"Mean Absolute Error\": mae,\n",
        "                     \"Mean Square Error\": mse,\n",
        "                     \"Root Mean Square Error\": np.sqrt(mse),\n",
        "                     \"Max Error\": max_err}\n",
        "\n",
        "    return model_results\n",
        "\n",
        "def calculate_results_scaled(y_true, y_pred):\n",
        "    inv_y_true = minmax_scaler.inverse_transform(y_true.reshape(-1, 1))\n",
        "    inv_y_pred = minmax_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
        "    model_results = calculate_results(inv_y_true, inv_y_pred)\n",
        "    \n",
        "    return model_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ydhk9wnIqyk6"
      },
      "outputs": [],
      "source": [
        "model_pred_probs = model.predict(valid_data)\n",
        "model_results = calculate_results(y_test, model_pred_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3bSo2ZRqyk6"
      },
      "outputs": [],
      "source": [
        "model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnEJVJGNqyk6"
      },
      "source": [
        "# Top 10% Wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHnTFaiZqyk7"
      },
      "outputs": [],
      "source": [
        "y_pred = model_pred_probs.squeeze()\n",
        "error = (y_test - y_pred).squeeze()\n",
        "square_error = np.square(error)\n",
        "\n",
        "validation_df = pd.DataFrame({'true': y_test,\n",
        "                             'pred': y_pred,\n",
        "                             'error': error,\n",
        "                             'square_error': square_error})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFIxSdYKqyk7"
      },
      "outputs": [],
      "source": [
        "top_100_wrong = validation_df.sort_values(\"error\", ascending=False).head(100)\n",
        "top_100_wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej3eUlIjqyk7"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlaD9eQtqyk8"
      },
      "outputs": [],
      "source": [
        "test_data = tf.data.Dataset.from_tensor_slices(test_df[\"image_hist\"].to_list()).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOO_MkGkqyk8"
      },
      "outputs": [],
      "source": [
        "model_predictions = model.predict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxvJDbeOqyk8"
      },
      "outputs": [],
      "source": [
        "model_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA7jYKOyqyk8"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame({\"id\": test_df[\"id\"], \"distance\": model_predictions.squeeze()})\n",
        "\n",
        "submission_df.to_csv(\"submission-best.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "tsp-cv-best.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0a38d37dc6a45ce77d3fdf67eee06a9a65b273241b27a8c0a288784bfb7a982d"
    },
    "kernelspec": {
      "display_name": "Python 3.8 (tensorflow)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
